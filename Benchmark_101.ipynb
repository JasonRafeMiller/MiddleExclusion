{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace02a34-7d06-4e75-a244-cf1b915a9471",
   "metadata": {
    "id": "ace02a34-7d06-4e75-a244-cf1b915a9471"
   },
   "source": [
    "# Benchmark\n",
    "Adopt the RNAlight code, mostly.\n",
    "\n",
    "Input files: one for cytoplasmic lncRNA, the other for nuclear lncRNA.   \n",
    "Input file format: tab-delimited lines of 3 fields: transcipt ID, gene name, RNA sequence.    \n",
    "Header line: ensembl_transcript_id name cdna \n",
    "Data lines: ENST00000371086\tDLEU2L\tGAAAGTTTTCACTGCATCT... \n",
    "Each lncRNA is placed in either file, depending on mean CNRCI over 14 cell lines from lncATLAS.   \n",
    "The threshold is zero; positive CNRCI values are cytoplasmic and others are nuclear.    \n",
    "Use the Ensembl transcript ID (prefix ENST) without any version number suffix.\n",
    "Use the canonical RNA sequence from GenCode to represent each gene in lncATLAS.\n",
    "Evaluate the model by cross-validation on the entire dataset (no test subset withheld)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1849059a-b4a8-4b24-b3b4-5ef3f15d5a2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1849059a-b4a8-4b24-b3b4-5ef3f15d5a2f",
    "outputId": "35e1b076-5ead-44ec-b2f7-6630f3696f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:36:23.929517\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f0998f-216d-441d-9dc5-5cf2827fe98c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86f0998f-216d-441d-9dc5-5cf2827fe98c",
    "outputId": "ee5219b1-5b71-41fc-b334-e8b44e5ebefb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "import lightgbm\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb7b34a-6cb1-4162-9c13-8d64163e98ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bb7b34a-6cb1-4162-9c13-8d64163e98ae",
    "outputId": "727d459f-1846-479a-83f6-a2b079a67652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a3c204-1130-447c-a654-c8bff965a3a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06a3c204-1130-447c-a654-c8bff965a3a8",
    "outputId": "0a0779bf-b291-4e57-f423-12f2ade5d267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA DIR ./\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './'    \n",
    "MODEL_DIR = './'    \n",
    "output_dir = './'\n",
    "print('DATA DIR', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295a8797-dd52-4b92-969a-0655f5e62310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "295a8797-dd52-4b92-969a-0655f5e62310",
    "outputId": "b83bb5d8-4c5d-490d-cf72-e8b5c8bc756c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "SEED = 100\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "print(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1ce357-cdc1-422e-aba2-8c8a995f73f8",
   "metadata": {
    "id": "4c1ce357-cdc1-422e-aba2-8c8a995f73f8"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    new_model = lightgbm.LGBMClassifier()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9eb695-32e6-4527-b54c-a1afd03bcdff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da9eb695-32e6-4527-b54c-a1afd03bcdff",
    "outputId": "5cc56bc1-9456-4fed-a015-983975c74220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier()\n"
     ]
    }
   ],
   "source": [
    "test = build_model()\n",
    "print(test)\n",
    "test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea5888f-521c-418b-ad28-437df4507c44",
   "metadata": {
    "id": "dea5888f-521c-418b-ad28-437df4507c44"
   },
   "outputs": [],
   "source": [
    "class stats_collector:\n",
    "    def __init__(self):\n",
    "        self.reset_statistics()\n",
    "    def reset_statistics(self):\n",
    "        self.cv_accuracy=[]\n",
    "        self.cv_precision=[]\n",
    "        self.cv_recall=[]\n",
    "        self.cv_f1=[]\n",
    "        self.cv_mcc=[]\n",
    "        self.cv_auprc=[]\n",
    "        self.cv_auroc=[]\n",
    "    def _append_statistics(self,accuracy,precision,recall,f1,mcc,auprc,auroc):\n",
    "        self.cv_accuracy.append(accuracy)\n",
    "        self.cv_precision.append(precision)\n",
    "        self.cv_recall.append(recall)\n",
    "        self.cv_f1.append(f1)\n",
    "        self.cv_mcc.append(mcc)\n",
    "        self.cv_auprc.append(auprc)\n",
    "        self.cv_auroc.append(auroc)\n",
    "    def compute_performance(self,y_test,yhat_pred,yhat_classes,verbose=False):\n",
    "        accuracy = accuracy_score(y_test, yhat_classes)*100.\n",
    "        precision = precision_score(y_test, yhat_classes)*100.\n",
    "        recall = recall_score(y_test, yhat_classes)*100.\n",
    "        f1 = f1_score(y_test, yhat_classes)*100.\n",
    "        mcc = matthews_corrcoef(y_test, yhat_classes)\n",
    "        prc_Y, prc_X, prc_bins = precision_recall_curve(y_test, yhat_pred)\n",
    "        auprc = auc(prc_X,prc_Y)*100.\n",
    "        auroc = roc_auc_score(y_test, yhat_pred)*100.\n",
    "        self._append_statistics(accuracy,precision,recall,f1,mcc,auprc,auroc)\n",
    "        if verbose:\n",
    "            self._show_confusion(y_test,yhat_pred,yhat_classes)\n",
    "            self._show_statistics(accuracy,precision,recall,f1,mcc,auprc,auroc)\n",
    "    def _show_confusion(self,y_test,yhat_pred,yhat_classes):\n",
    "            print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
    "            print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
    "            cm1 = confusion_matrix(y_test,yhat_classes)\n",
    "            print('Confusion matrix\\n',cm1)\n",
    "            cm2 = confusion_matrix(y_test,yhat_classes,normalize='all')\n",
    "            print('Normalized matrix\\n',cm2)\n",
    "    def _show_statistics(self,accuracy,precision,recall,f1,mcc,auprc,auroc):\n",
    "            print('accuracy:',accuracy,'precision:',precision,'recall:',recall,\\\n",
    "                  'F1:',f1,'MCC:',mcc,'AUPRC:',auprc,'AUROC:',auroc)\n",
    "    def _show_variance(self, name, stats_list):\n",
    "        if name=='MCC':\n",
    "            print('%10s %5.3f mean, %6.3f stdev' % (name,np.mean(stats_list),np.std(stats_list) ) )\n",
    "        else:\n",
    "            print('%10s %5.2f mean, %6.3f stdev' % (name,np.mean(stats_list),np.std(stats_list) ) )\n",
    "        print(stats_list)\n",
    "    def dump_all(self):\n",
    "        self._show_variance('accuracy', self.cv_accuracy)\n",
    "        self._show_variance('precision',self.cv_precision)\n",
    "        self._show_variance('recall',   self.cv_recall)\n",
    "        self._show_variance('F1',       self.cv_f1)\n",
    "        self._show_variance('MCC',      self.cv_mcc)\n",
    "        self._show_variance('AUPRC',    self.cv_auprc)\n",
    "        self._show_variance('AUROC',    self.cv_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c417401f-299a-4fbe-b866-00ad89f0e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/2970520/string-count-with-overlapping-occurrences\n",
    "# Count matching substrings including overlapping ones\n",
    "def occurrences(string, sub):\n",
    "    count = start = 0\n",
    "    while True:\n",
    "        start = string.find(sub, start) + 1\n",
    "        if start > 0:\n",
    "            count+=1\n",
    "        else:\n",
    "            return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f11ef7-acaf-4275-a736-4505bc5cb2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "rna='AAAAA'\n",
    "mer='AAA'\n",
    "print(rna.count(mer))\n",
    "print(occurrences('AAAAA','AAA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0fe64b-d172-4167-bc2b-d9f617fbb1a1",
   "metadata": {
    "id": "2c0fe64b-d172-4167-bc2b-d9f617fbb1a1"
   },
   "outputs": [],
   "source": [
    "# From RNAlight notebook\n",
    "def _count_kmer(Dataset,k): # k = 3,4,5\n",
    "\n",
    "    # copy dataset\n",
    "    dataset = copy.deepcopy(Dataset)\n",
    "    # alphbet of nucleotide\n",
    "    nucleotide = ['A','C','G','T']\n",
    "\n",
    "    # generate k-mers\n",
    "    #  k == 5:\n",
    "    five = list(itertools.product(nucleotide,repeat=5))\n",
    "    pentamer = []\n",
    "    for n in five:\n",
    "        pentamer.append(\"\".join(n))\n",
    "\n",
    "    #  k == 4:\n",
    "    four = list(itertools.product(nucleotide,repeat=4))\n",
    "    tetramer = []\n",
    "    for n in four:\n",
    "        tetramer.append(\"\".join(n))\n",
    "\n",
    "    # k == 3:\n",
    "    three = list(itertools.product(nucleotide,repeat=3))\n",
    "    threemer = []\n",
    "    for n in three:\n",
    "        threemer.append(\"\".join(n))\n",
    "\n",
    "    # input features can be combinations of diffrent k values\n",
    "    if k == 34:\n",
    "        table_kmer = dict.fromkeys(threemer,0)\n",
    "        table_kmer.update(dict.fromkeys(tetramer,0))\n",
    "    if k == 45:\n",
    "        table_kmer = dict.fromkeys(tetramer,0)\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "    if k == 345:\n",
    "        table_kmer = dict.fromkeys(threemer,0)\n",
    "        table_kmer.update(dict.fromkeys(tetramer,0))\n",
    "        table_kmer.update(dict.fromkeys(pentamer,0))\n",
    "\n",
    "    # count k-mer for each sequence\n",
    "    for mer in table_kmer.keys():\n",
    "        #table_kmer[mer] = dataset[\"cdna\"].apply(lambda x : x.count(mer))\n",
    "        table_kmer[mer] = dataset[\"cdna\"].apply(lambda x : occurrences(x,mer))\n",
    "\n",
    "    # for k-mer raw count without normalization, index: nuc:1 or cyto:0\n",
    "    rawcount_kmer_df = pd.DataFrame(table_kmer)\n",
    "    df1_rawcount = pd.concat([rawcount_kmer_df,dataset[\"ensembl_transcript_id\"]],axis = 1)\n",
    "    df1_rawcount.index = dataset[\"tag\"]\n",
    "\n",
    "    # for k-mer frequency with normalization , index: nuc:1 or cyto:0\n",
    "    freq_kmer_df = rawcount_kmer_df.apply(lambda x: x/x.sum(),axis=1)\n",
    "    df1 = pd.concat([freq_kmer_df,dataset[\"ensembl_transcript_id\"]],axis = 1)\n",
    "    df1.index = dataset[\"tag\"]\n",
    "\n",
    "    return df1  # ,df1_rawcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e70228e-003b-4b4e-895f-b5d642270490",
   "metadata": {
    "id": "9e70228e-003b-4b4e-895f-b5d642270490"
   },
   "outputs": [],
   "source": [
    "# From RNAlight notebook\n",
    "def load_dataframe(cyto_f,nuc_f):\n",
    "    print('load dataframe')\n",
    "    dataset_cyto = pd.read_csv(cyto_f,sep='\\t',index_col = False)    #1806\n",
    "    dataset_nuc = pd.read_csv(nuc_f,sep='\\t',index_col = False)    #1986\n",
    "    print( len(dataset_cyto), 'cytoplasmic samples')\n",
    "    print( len(dataset_nuc),  'nuclear samples')\n",
    "    return dataset_cyto,dataset_nuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449b9c12-3a7d-4e97-9816-43db7ea0caa8",
   "metadata": {
    "id": "449b9c12-3a7d-4e97-9816-43db7ea0caa8"
   },
   "outputs": [],
   "source": [
    "# Added\n",
    "def rebalance(dataset_cyto,dataset_nuc):\n",
    "    print('sample down to balance classes')\n",
    "    min_size = min(len(dataset_cyto),len(dataset_nuc))\n",
    "    # random sampling without replacement\n",
    "    dataset_cyto = dataset_cyto.sample(min_size, random_state=SEED)\n",
    "    dataset_nuc  = dataset_nuc.sample(min_size,  random_state=SEED)\n",
    "    print( len(dataset_cyto), 'cytoplasmic samples')\n",
    "    print( len(dataset_nuc),  'nuclear samples')\n",
    "    return dataset_cyto,dataset_nuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ef1e58c-07a6-47f0-9fef-c439c298b3ff",
   "metadata": {
    "id": "4ef1e58c-07a6-47f0-9fef-c439c298b3ff"
   },
   "outputs": [],
   "source": [
    "# From RNAlight notebook\n",
    "def extract_features_and_split(dataset_cyto,dataset_nuc):\n",
    "    print('add labels, concatenate')\n",
    "    # Set the tag of RCI(log2FC): nuclear 1 / cytosol 0\n",
    "    dataset_nuc['tag'] = 1;dataset_cyto['tag'] = 0\n",
    "    # merge the nuc and cyto dataset\n",
    "    dataset = pd.concat([dataset_nuc,dataset_cyto]) # 3792\n",
    "\n",
    "    print('dedupe (probably not necessary)')\n",
    "    # remove duplications(actually,each lncRNA is unique in its class)\n",
    "    dataset.drop_duplicates(keep=\"first\",subset=[\"ensembl_transcript_id\",\"name\",\"cdna\"],inplace=True) # 3792\n",
    "\n",
    "    print('count kmers')\n",
    "    # k = 3,4,5 count the normalized and raw count of kmer\n",
    "    df_kmer_345 = _count_kmer(dataset,345)   # df_kmer_345,df_kmer_345_rawcount =\n",
    "\n",
    "    # We commented this out. No need to save the tsv.\n",
    "    # df_kmer_345.to_csv(os.path.join(output_dir,\"df_kmer345_freq.tsv\"),sep='\\t')\n",
    "    # This was commented out in the original. Seems they reran using saved kmers. Should test if file exists.\n",
    "    # load kmer file\n",
    "    # df_kmer_345 = pd.read_csv(os.path.join(output_dir,\"df_kmer345_freq.tsv\"),sep='\\t',index_col= 0)\n",
    "\n",
    "    # convert to x:kmer-freq , y:label\n",
    "    del df_kmer_345['ensembl_transcript_id']\n",
    "    x_kmer = df_kmer_345.values\n",
    "    y_kmer = y_kmer = np.array(df_kmer_345.index)\n",
    "\n",
    "    # split into training and test sets (9:1)\n",
    "    #print('train/test split')\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x_kmer, y_kmer, test_size = 0.1, random_state = SEED)\n",
    "\n",
    "    # Use all the data\n",
    "    print('Apply cross-validation to all the data (no test set withheld)')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_kmer, y_kmer, test_size = None, random_state = SEED)\n",
    "\n",
    "    print('train set shape',x_train.shape)\n",
    "    # Added\n",
    "    labels,counts = np.unique(y_train,return_counts=True)\n",
    "    print('train set labels', labels, 'counts',counts)\n",
    "    labels,counts = np.unique(y_test,return_counts=True)\n",
    "    print('test set labels', labels, 'counts',counts)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b66e70-43ab-4a04-95ec-8d74064d8b65",
   "metadata": {
    "id": "18b66e70-43ab-4a04-95ec-8d74064d8b65"
   },
   "outputs": [],
   "source": [
    "def do_cv(x_train, y_train):\n",
    "    stats = stats_collector()\n",
    "    for round in range(1,3):\n",
    "        fold=0\n",
    "        splitter = KFold(n_splits=5)\n",
    "        for train_index, valid_index in splitter.split(x_train):\n",
    "            fold += 1\n",
    "            print('Round', round, 'Fold', fold)\n",
    "            print('Num samples in train and valid sets:', len(train_index), len(valid_index))\n",
    "            print('Train')\n",
    "            lgb = build_model()\n",
    "            history = lgb.fit(x_train[train_index], y_train[train_index])\n",
    "            print('Validate')\n",
    "            x_valid = x_train[valid_index]\n",
    "            y_valid = y_train[valid_index]\n",
    "            yhat_classes= lgb.predict(x_valid)  # get 0 or 1\n",
    "            yhat_pairs=   lgb.predict_proba(x_valid)  # get [ prob of 0, prob of 1 ]\n",
    "            yhat_pred=    [pair[1] for pair in yhat_pairs]\n",
    "            stats.compute_performance(y_valid,yhat_pred,yhat_classes,verbose=False)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169b5c9-1a27-49d9-a998-628b3673bec2",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "336edb33-e974-488e-82cb-f5d8bf6bab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:46:23.819294\n",
      "Use our lncATLAS training set\n",
      "./mean_RCI_positive.canonical.tsv \n",
      " ./mean_RCI_negative.canonical.tsv\n",
      "load dataframe\n",
      "1701 cytoplasmic samples\n",
      "2835 nuclear samples\n",
      "2024-03-24 19:46:23.996776\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print('Use our lncATLAS training set')\n",
    "cyt_file  = DATA_DIR+'mean_RCI_positive.canonical.tsv'   \n",
    "nuc_file  = DATA_DIR+'mean_RCI_negative.canonical.tsv'\n",
    "print(cyt_file,'\\n',nuc_file)\n",
    "dataset_cyto,dataset_nuc = load_dataframe(cyt_file,nuc_file)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1830d712-20a6-4f94-9ddb-e21be989c4d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1830d712-20a6-4f94-9ddb-e21be989c4d6",
    "outputId": "205d7595-385e-4ad4-ca01-7c38e166a1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:46:24.612879\n",
      "add labels, concatenate\n",
      "dedupe (probably not necessary)\n",
      "count kmers\n",
      "Apply cross-validation to all the data (no test set withheld)\n",
      "train set shape (3402, 1344)\n",
      "train set labels [0 1] counts [1272 2130]\n",
      "test set labels [0 1] counts [429 705]\n",
      "2024-03-24 19:47:14.651765\n",
      "Round 1 Fold 1\n",
      "Num samples in train and valid sets: 2721 681\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1677, number of negative: 1044\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096581 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339187\n",
      "[LightGBM] [Info] Number of data points in the train set: 2721, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.616318 -> initscore=0.473947\n",
      "[LightGBM] [Info] Start training from score 0.473947\n",
      "Validate\n",
      "Round 1 Fold 2\n",
      "Num samples in train and valid sets: 2721 681\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1715, number of negative: 1006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106768 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339233\n",
      "[LightGBM] [Info] Number of data points in the train set: 2721, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.630283 -> initscore=0.533431\n",
      "[LightGBM] [Info] Start training from score 0.533431\n",
      "Validate\n",
      "Round 1 Fold 3\n",
      "Num samples in train and valid sets: 2722 680\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1700, number of negative: 1022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339100\n",
      "[LightGBM] [Info] Number of data points in the train set: 2722, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624541 -> initscore=0.508867\n",
      "[LightGBM] [Info] Start training from score 0.508867\n",
      "Validate\n",
      "Round 1 Fold 4\n",
      "Num samples in train and valid sets: 2722 680\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1727, number of negative: 995\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339326\n",
      "[LightGBM] [Info] Number of data points in the train set: 2722, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.634460 -> initscore=0.551398\n",
      "[LightGBM] [Info] Start training from score 0.551398\n",
      "Validate\n",
      "Round 1 Fold 5\n",
      "Num samples in train and valid sets: 2722 680\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1701, number of negative: 1021\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339228\n",
      "[LightGBM] [Info] Number of data points in the train set: 2722, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624908 -> initscore=0.510434\n",
      "[LightGBM] [Info] Start training from score 0.510434\n",
      "Validate\n",
      "Round 2 Fold 1\n",
      "Num samples in train and valid sets: 2721 681\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1677, number of negative: 1044\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339187\n",
      "[LightGBM] [Info] Number of data points in the train set: 2721, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.616318 -> initscore=0.473947\n",
      "[LightGBM] [Info] Start training from score 0.473947\n",
      "Validate\n",
      "Round 2 Fold 2\n",
      "Num samples in train and valid sets: 2721 681\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1715, number of negative: 1006\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339233\n",
      "[LightGBM] [Info] Number of data points in the train set: 2721, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.630283 -> initscore=0.533431\n",
      "[LightGBM] [Info] Start training from score 0.533431\n",
      "Validate\n",
      "Round 2 Fold 3\n",
      "Num samples in train and valid sets: 2722 680\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1700, number of negative: 1022\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339100\n",
      "[LightGBM] [Info] Number of data points in the train set: 2722, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624541 -> initscore=0.508867\n",
      "[LightGBM] [Info] Start training from score 0.508867\n",
      "Validate\n",
      "Round 2 Fold 4\n",
      "Num samples in train and valid sets: 2722 680\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1727, number of negative: 995\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339326\n",
      "[LightGBM] [Info] Number of data points in the train set: 2722, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.634460 -> initscore=0.551398\n",
      "[LightGBM] [Info] Start training from score 0.551398\n",
      "Validate\n",
      "Round 2 Fold 5\n",
      "Num samples in train and valid sets: 2722 680\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1701, number of negative: 1021\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 339228\n",
      "[LightGBM] [Info] Number of data points in the train set: 2722, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.624908 -> initscore=0.510434\n",
      "[LightGBM] [Info] Start training from score 0.510434\n",
      "Validate\n",
      "2024-03-24 19:49:34.612705\n",
      "\n",
      "Cross validation results\n",
      "  accuracy 67.05 mean,  0.661 stdev\n",
      "[67.1071953010279, 67.54772393538914, 67.79411764705883, 65.88235294117646, 66.91176470588235, 67.1071953010279, 67.54772393538914, 67.79411764705883, 65.88235294117646, 66.91176470588235]\n",
      " precision 69.82 mean,  2.521 stdev\n",
      "[73.22515212981745, 68.30188679245282, 71.05788423153693, 65.86270871985158, 70.64777327935222, 73.22515212981745, 68.30188679245282, 71.05788423153693, 65.86270871985158, 70.64777327935222]\n",
      "    recall 83.83 mean,  3.288 stdev\n",
      "[79.69094922737307, 87.2289156626506, 82.7906976744186, 88.08933002481389, 81.35198135198135, 79.69094922737307, 87.2289156626506, 82.7906976744186, 88.08933002481389, 81.35198135198135]\n",
      "        F1 76.08 mean,  0.492 stdev\n",
      "[76.32135306553911, 76.61375661375662, 76.47690655209452, 75.3715498938429, 75.62296858071505, 76.32135306553911, 76.61375661375662, 76.47690655209452, 75.3715498938429, 75.62296858071505]\n",
      "       MCC 0.260 mean,  0.018 stdev\n",
      "[0.230088636939329, 0.2827046608796745, 0.27142359774620206, 0.26255109224686773, 0.255296970853849, 0.230088636939329, 0.2827046608796745, 0.27142359774620206, 0.26255109224686773, 0.255296970853849]\n",
      "     AUPRC 77.94 mean,  1.113 stdev\n",
      "[79.25193838983691, 78.04732175171397, 78.77233260991292, 76.02780827250152, 77.60902859029115, 79.25193838983691, 78.04732175171397, 78.77233260991292, 76.02780827250152, 77.60902859029115]\n",
      "     AUROC 68.57 mean,  0.934 stdev\n",
      "[67.14980054993998, 69.58148383005707, 69.15441860465116, 69.18149976261074, 67.79316301228653, 67.14980054993998, 69.58148383005707, 69.15441860465116, 69.18149976261074, 67.79316301228653]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    print(datetime.now())\n",
    "    x_train, x_test, y_train, y_test = extract_features_and_split(dataset_cyto,dataset_nuc)\n",
    "    print(datetime.now())\n",
    "    stats = do_cv(x_train, y_train)\n",
    "    print(datetime.now())\n",
    "    print('\\nCross validation results')\n",
    "    stats.dump_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44560340-f786-4111-a3de-242d9b2aeb17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44560340-f786-4111-a3de-242d9b2aeb17",
    "outputId": "6053dce0-01c3-474b-df55-a7bb89027670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:49:34.635057\n",
      "Rebalance (sample down the majority class) and repeat\n",
      "sample down to balance classes\n",
      "1701 cytoplasmic samples\n",
      "1701 nuclear samples\n",
      "2024-03-24 19:49:34.668199\n",
      "add labels, concatenate\n",
      "dedupe (probably not necessary)\n",
      "count kmers\n",
      "Apply cross-validation to all the data (no test set withheld)\n",
      "train set shape (2551, 1344)\n",
      "train set labels [0 1] counts [1294 1257]\n",
      "test set labels [0 1] counts [407 444]\n",
      "2024-03-24 19:50:11.739582\n",
      "Round 1 Fold 1\n",
      "Num samples in train and valid sets: 2040 511\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1000, number of negative: 1040\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2040, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490196 -> initscore=-0.039221\n",
      "[LightGBM] [Info] Start training from score -0.039221\n",
      "Validate\n",
      "Round 1 Fold 2\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 983, number of negative: 1058\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332437\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.481627 -> initscore=-0.073526\n",
      "[LightGBM] [Info] Start training from score -0.073526\n",
      "Validate\n",
      "Round 1 Fold 3\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1017, number of negative: 1024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332285\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498285 -> initscore=-0.006859\n",
      "[LightGBM] [Info] Start training from score -0.006859\n",
      "Validate\n",
      "Round 1 Fold 4\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1020, number of negative: 1021\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332693\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499755 -> initscore=-0.000980\n",
      "[LightGBM] [Info] Start training from score -0.000980\n",
      "Validate\n",
      "Round 1 Fold 5\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1008, number of negative: 1033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332292\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493876 -> initscore=-0.024499\n",
      "[LightGBM] [Info] Start training from score -0.024499\n",
      "Validate\n",
      "Round 2 Fold 1\n",
      "Num samples in train and valid sets: 2040 511\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1000, number of negative: 1040\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2040, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490196 -> initscore=-0.039221\n",
      "[LightGBM] [Info] Start training from score -0.039221\n",
      "Validate\n",
      "Round 2 Fold 2\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 983, number of negative: 1058\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332437\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.481627 -> initscore=-0.073526\n",
      "[LightGBM] [Info] Start training from score -0.073526\n",
      "Validate\n",
      "Round 2 Fold 3\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1017, number of negative: 1024\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332285\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498285 -> initscore=-0.006859\n",
      "[LightGBM] [Info] Start training from score -0.006859\n",
      "Validate\n",
      "Round 2 Fold 4\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1020, number of negative: 1021\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332693\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499755 -> initscore=-0.000980\n",
      "[LightGBM] [Info] Start training from score -0.000980\n",
      "Validate\n",
      "Round 2 Fold 5\n",
      "Num samples in train and valid sets: 2041 510\n",
      "Train\n",
      "[LightGBM] [Info] Number of positive: 1008, number of negative: 1033\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 332292\n",
      "[LightGBM] [Info] Number of data points in the train set: 2041, number of used features: 1344\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493876 -> initscore=-0.024499\n",
      "[LightGBM] [Info] Start training from score -0.024499\n",
      "Validate\n",
      "2024-03-24 19:52:09.979711\n",
      "\n",
      "Cross validation results\n",
      "  accuracy 60.88 mean,  1.720 stdev\n",
      "[60.273972602739725, 63.921568627450974, 60.98039215686275, 60.588235294117645, 58.62745098039216, 60.273972602739725, 63.921568627450974, 60.98039215686275, 60.588235294117645, 58.62745098039216]\n",
      " precision 60.51 mean,  4.223 stdev\n",
      "[61.065573770491795, 68.44262295081968, 58.50622406639005, 56.56934306569343, 57.98319327731093, 61.065573770491795, 68.44262295081968, 58.50622406639005, 56.56934306569343, 57.98319327731093]\n",
      "    recall 59.70 mean,  3.354 stdev\n",
      "[57.97665369649806, 60.94890510948905, 58.75, 65.40084388185655, 55.42168674698795, 57.97665369649806, 60.94890510948905, 58.75, 65.40084388185655, 55.42168674698795]\n",
      "        F1 59.99 mean,  2.597 stdev\n",
      "[59.48103792415169, 64.47876447876449, 58.62785862785863, 60.66536203522506, 56.673511293634505, 59.48103792415169, 64.47876447876449, 58.62785862785863, 60.66536203522506, 56.673511293634505]\n",
      "       MCC 0.219 mean,  0.036 stdev\n",
      "[0.20595597000150298, 0.28269374256298413, 0.2170810598259909, 0.21817339430607727, 0.17140907526253968, 0.20595597000150298, 0.28269374256298413, 0.2170810598259909, 0.21817339430607727, 0.17140907526253968]\n",
      "     AUPRC 65.77 mean,  3.058 stdev\n",
      "[64.50585185310032, 71.72685684258366, 65.29632494049535, 63.114651152072334, 64.22561276920669, 64.50585185310032, 71.72685684258366, 65.29632494049535, 63.114651152072334, 64.22561276920669]\n",
      "     AUROC 66.66 mean,  1.327 stdev\n",
      "[65.22258647630134, 68.77087714957317, 67.50462962962963, 66.37764485865752, 65.4403052824324, 65.22258647630134, 68.77087714957317, 67.50462962962963, 66.37764485865752, 65.4403052824324]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    print(datetime.now())\n",
    "    print('Rebalance (sample down the majority class) and repeat')\n",
    "    dataset_cyto,dataset_nuc = rebalance(dataset_cyto,dataset_nuc)\n",
    "    print(datetime.now())\n",
    "    x_train, x_test, y_train, y_test = extract_features_and_split(dataset_cyto,dataset_nuc)\n",
    "    print(datetime.now())\n",
    "    stats = do_cv(x_train, y_train)\n",
    "    print(datetime.now())\n",
    "    print('\\nCross validation results')\n",
    "    stats.dump_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35cf8437-c2c6-41d2-8b1f-70cc7ba25672",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35cf8437-c2c6-41d2-8b1f-70cc7ba25672",
    "outputId": "50d657e7-1f49-43ff-fccf-13364c8562c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:52:09.986689\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88227b8a-512a-4228-869b-2d9ed94f5dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
